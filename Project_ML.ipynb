{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b3038fd7-c9d5-4faf-b17b-876da0b5f4ac",
   "metadata": {},
   "source": [
    "# ML Single Cell Classification Project\n",
    "\n",
    "In this project, we build a classifier to predict brain cell types using single-cell RNA sequencing data. The dataset consists of:\n",
    "- **Expression data:** A normalized and partially preprocessed gene expression matrix (280,186 cells × 254 genes) stored in `counts.h5ad`\n",
    "- **Cell annotations:** A CSV file (`cell_labels.csv`) containing cell type labels and additional metadata  \n",
    "  (Three classes: **GABAergic**, **Glutamatergic**, and **Other**)\n",
    "\n",
    "We use the `scanpy` library for handling single-cell data, along with common machine learning libraries from scikit-learn for model training, hyperparameter tuning, and evaluation.\n",
    "\n",
    "---\n",
    "\n",
    "## 1. Environment Setup and Imports\n",
    "\n",
    "First, install and load the required packages. (If working in Google Colab, install `scanpy` with `!pip install scanpy`.)\n",
    "\n",
    "```python\n",
    "# Install scanpy if needed\n",
    "# !pip install scanpy\n",
    "\n",
    "import scanpy as sc\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import anndata as ad\n",
    "import pooch \n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.utils import resample\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import f1_score\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd67d0a5",
   "metadata": {},
   "source": [
    "## 2. Data Loading and Exploratory Data Analysis\n",
    "\n",
    "**2.1. Load Annotation and Expression Data**\n",
    "\n",
    "The cell annotation file (`cell_labels.csv`) contains the class labels (in column class_label), while the expression data (`counts.h5ad`) contains the gene expression matrix and metadata.\n",
    "\n",
    "```python\n",
    "#Load cell annotation file (cell labels)\n",
    "cell_labels = pd.read_csv(\"C:/Users/Семья/Desktop/bioinformatics/ML/project/cell_labels.csv\", index_col=0)\n",
    "\n",
    "#Load expression data (scanpy format)\n",
    "adata = sc.read_h5ad(\"C:/Users/Семья/Desktop/bioinformatics/ML/project/counts.h5ad\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2579be37",
   "metadata": {},
   "source": [
    "**2.2. Explore the Annotation Data**\n",
    "\n",
    "Display the first few rows and get a summary of class labels.\n",
    "```python\n",
    "# Display first rows of cell_labels\n",
    "cell_labels.head()\n",
    "\n",
    "# Extract class labels and print unique classes\n",
    "class_label = cell_labels['class_label'].values  \n",
    "print(f\"Unique classes: {np.unique(class_label)}\")\n",
    "```\n",
    "Expected output:\n",
    "Unique classes: ['GABAergic', 'Glutamatergic', 'Other']\n",
    "\n",
    "\n",
    "Check the distribution of cell types:\n",
    "\n",
    "```python\n",
    "# Count of cells in each class\n",
    "print(cell_labels['class_label'].value_counts())\n",
    "\n",
    "# Group by class label for further insights\n",
    "cell_labels.groupby('class_label').count()\n",
    "```\n",
    "\n",
    "Visualize the distribution of subclasses (if available) with a bar plot:\n",
    "\n",
    "```python\n",
    "cell_labels['subclass'].value_counts().plot(kind='bar')\n",
    "plt.title(\"Subclass Distribution\")\n",
    "plt.xlabel(\"Subclass\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.show()\n",
    "```\n",
    "Create a comparison between subclass and label:\n",
    "\n",
    "```python\n",
    "# Check if 'subclass' matches 'label'\n",
    "cell_labels['is_equal'] = cell_labels['subclass'] == cell_labels['label']\n",
    "counts = cell_labels['is_equal'].value_counts()\n",
    "\n",
    "plt.figure(figsize=(6, 4))\n",
    "sns.barplot(x=counts.index, y=counts.values, palette=\"pastel\")\n",
    "plt.xticks([0, 1], ['Not Equal', 'Equal'])\n",
    "plt.ylabel(\"Count\")\n",
    "plt.title(\"Comparison of Subclass and Label\")\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "Visualize the mapping of subclasses to class labels using heatmap:\n",
    "```python\n",
    "cross_tab = pd.crosstab(cell_labels['class_label'], cell_labels['subclass'])\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.heatmap(cross_tab, annot=False, cmap=\"Blues\", cbar=True)\n",
    "plt.title(\"Mapping of Subclasses to Class Labels\", fontsize=16)\n",
    "plt.xlabel(\"Subclass\", fontsize=12)\n",
    "plt.ylabel(\"Class Label\", fontsize=12)\n",
    "plt.xticks(rotation=45, ha=\"right\")\n",
    "plt.yticks(rotation=0)\n",
    "plt.show()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "675e762f",
   "metadata": {},
   "source": [
    "## 3. Data Preprocessing with Scanpy\n",
    "\n",
    "**3.1. Explore the scRNAseq Data**\n",
    "\n",
    "Display key information about the `adata` object:\n",
    "\n",
    "```python\n",
    "# Print basic details about the AnnData object\n",
    "print(adata)\n",
    "adata.X        # Expression matrix\n",
    "adata.var      # Gene information\n",
    "adata.obs      # Cell metadata\n",
    "adata.layers.keys()  # Other available data layers\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0b9298b",
   "metadata": {},
   "source": [
    "**3.2. Quality Control (QC) Metrics**\n",
    "\n",
    "Calculate quality control metrics using Scanpy's built-in function:\n",
    "\n",
    "```python\n",
    "# Calculate QC metrics and add to adata.obs and adata.var\n",
    "obs_qc, var_qc = sc.pp.calculate_qc_metrics(adata, percent_top=[20,50,100], inplace=False)\n",
    "print(obs_qc.columns)\n",
    "print(var_qc.columns)\n",
    "```\n",
    "\n",
    "Visualize QC metrics:\n",
    "\n",
    "```python\n",
    "# Add QC metrics to the adata object\n",
    "sc.pp.calculate_qc_metrics(adata, percent_top=[20,50,100], inplace=True)\n",
    "\n",
    "# Violin plot for several QC metrics\n",
    "sc.pl.violin(\n",
    "    adata,\n",
    "    [\"n_genes_by_counts\", \"log1p_n_genes_by_counts\", \"total_counts\", \"pct_counts_in_top_100_genes\",\n",
    "     \"pct_counts_in_top_50_genes\", \"pct_counts_in_top_20_genes\"],\n",
    "    jitter=0.4,\n",
    "    multi_panel=True,\n",
    ")\n",
    "```\n",
    "\n",
    "Scatter plot to check the relationship between total counts and number of genes:\n",
    "\n",
    "```python\n",
    "sc.pl.scatter(adata, \"total_counts\", \"n_genes_by_counts\")\n",
    "```\n",
    "\n",
    "Filter out cells with extremely low expression:\n",
    "\n",
    "```python\n",
    "# Filter cells with fewer than 1 gene detected (removes cells with zero counts)\n",
    "sc.pp.filter_cells(adata, min_genes=1)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f4bbd1f",
   "metadata": {},
   "source": [
    "**3.3. Integrate Annotation Data with adata**\n",
    "\n",
    "Join cell annotation data into `adata.obs`:\n",
    "\n",
    "\n",
    "```python\n",
    "adata.obs = adata.obs.join(cell_labels, how='left', rsuffix=\"_cell_labels\")\n",
    "\n",
    "# Drop unnecessary columns from the joined metadata\n",
    "cols_to_drop = [\"sample_id_cell_labels\", \"slice_id_cell_labels\", \"class_label_cell_labels\", \n",
    "                \"subclass_cell_labels\", \"label_cell_labels\"]\n",
    "adata.obs = adata.obs.drop(columns=cols_to_drop)\n",
    "```\n",
    "\n",
    "(Optional) Check for duplicate cells or doublets:\n",
    "\n",
    "```python\n",
    "sc.pp.scrublet(adata, batch_key=\"sample_id\")  # This computes doublet scores, if applicable\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21e24395",
   "metadata": {},
   "source": [
    "## 4. Defining Features and Labels for Classification\n",
    "\n",
    "**4.1. Define Input (X) and Output (y)**\n",
    "\n",
    "Extract the expression matrix as features and convert class labels to categorical numeric codes:\n",
    "\n",
    "```python\n",
    "# Define X as the expression matrix (cells × genes)\n",
    "X = adata.X\n",
    "\n",
    "# Convert class labels to categorical and then to numeric codes\n",
    "print(\"Unique class labels:\", adata.obs['class_label'].unique())\n",
    "adata.obs['class_label'] = pd.Categorical(\n",
    "    adata.obs['class_label'],\n",
    "    categories=['Other', 'Glutamatergic', 'GABAergic']\n",
    ")\n",
    "y = adata.obs['class_label'].cat.codes.values\n",
    "print(\"Categories:\", adata.obs['class_label'].cat.categories)\n",
    "```\n",
    "\n",
    "Check a few predicted labels:\n",
    "\n",
    "```python\n",
    "_labels = [adata.obs['class_label'].cat.categories[code] for code in y[:10]]\n",
    "print(\"First 10 cell type predictions:\", _labels)\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99399cd9",
   "metadata": {},
   "source": [
    "**4.2. Split the Data into Training, Validation, and Test Sets**\n",
    "\n",
    "Use stratified splitting to preserve class distributions:\n",
    "\n",
    "```python\n",
    "# First, split into training/validation and test sets (80/20)\n",
    "X_trainval, X_test, y_trainval, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, stratify=y, random_state=42\n",
    ")\n",
    "# Then, split the training/validation set into training and validation (75/25)\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_trainval, y_trainval, test_size=0.25, stratify=y_trainval, random_state=42\n",
    ")\n",
    "```\n",
    "\n",
    "Set up cross-validation to preserve the stratification:\n",
    "```python\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "```\n",
    "\n",
    "Normalize the feature data:\n",
    "```python\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_val = scaler.transform(X_val)\n",
    "X_test = scaler.transform(X_test)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3027c408",
   "metadata": {},
   "source": [
    "## 5. Model Training, Hyperparameter Tuning, and Evaluation\n",
    "\n",
    "**5.1. Define Models and Pipelines**\n",
    "\n",
    "Set up a dictionary with pipelines and parameter grids for multiple classifiers:\n",
    "\n",
    "```python\n",
    "models_params = {\n",
    "    \"KNN\": {\n",
    "        \"pipeline\": Pipeline([(\"clf\", KNeighborsClassifier())]),\n",
    "        \"params\": {\n",
    "            \"clf__n_neighbors\": [3, 5, 7, 10],\n",
    "            \"clf__weights\": [\"uniform\", \"distance\"]\n",
    "        }\n",
    "    },\n",
    "    \"LogisticRegression\": {\n",
    "        \"pipeline\": Pipeline([(\"clf\", LogisticRegression(random_state=42))]),\n",
    "        \"params\": {\n",
    "            \"clf__penalty\": [\"l2\", \"none\"],\n",
    "            \"clf__solver\": [\"lbfgs\", \"saga\", \"liblinear\"],\n",
    "            \"clf__max_iter\": [100, 200, 500],\n",
    "            \"clf__multi_class\": [\"auto\", \"ovr\", \"multinomial\"],\n",
    "            \"clf__n_jobs\": [-1]\n",
    "        }\n",
    "    },\n",
    "    \"RandomForest\": {\n",
    "        \"pipeline\": Pipeline([(\"clf\", RandomForestClassifier(random_state=42))]),\n",
    "        \"params\": {\n",
    "            \"clf__n_estimators\": [100, 200, 500],\n",
    "            \"clf__max_depth\": [None, 10, 20],\n",
    "            \"clf__min_samples_leaf\": [1, 2, 5]\n",
    "        }\n",
    "    },\n",
    "    \"SVM\": {\n",
    "        \"pipeline\": Pipeline([(\"clf\", SVC(random_state=42))]),\n",
    "        \"params\": {\n",
    "            \"clf__kernel\": [\"linear\", \"rbf\"],\n",
    "            \"clf__C\": [0.1, 1, 10]\n",
    "        }\n",
    "    }\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d70e5190",
   "metadata": {},
   "source": [
    "**5.2. Hyperparameter Tuning and Model Evaluation**\n",
    "\n",
    "Loop through each model, perform grid search with cross-validation, and evaluate performance on validation and test sets:\n",
    "\n",
    "```python\n",
    "results = []  # To store results\n",
    "\n",
    "for model_name, mp in models_params.items():\n",
    "    print(f\"Running GridSearch for model: {model_name}\")\n",
    "    grid_search = GridSearchCV(\n",
    "        estimator=mp[\"pipeline\"],\n",
    "        param_grid=mp[\"params\"],\n",
    "        cv=cv,\n",
    "        scoring=\"f1_macro\",  #F1-score metric\n",
    "        n_jobs=-1,\n",
    "        return_train_score=True\n",
    "    )\n",
    "    # Train the model\n",
    "    grid_search.fit(X_train, y_train)\n",
    "    \n",
    "    # Best parameters and cross-validation score\n",
    "    best_params = grid_search.best_params_\n",
    "    best_cv_score = grid_search.best_score_\n",
    "    \n",
    "    # Evaluate on the validation set\n",
    "    val_pred = grid_search.predict(X_val)\n",
    "    val_score = f1_score(y_val, val_pred, average=\"macro\")\n",
    "    \n",
    "    # Evaluate on the test set\n",
    "    test_pred = grid_search.predict(X_test)\n",
    "    test_score = f1_score(y_test, test_pred, average=\"macro\")\n",
    "    \n",
    "    results.append({\n",
    "        \"model\": model_name,\n",
    "        \"best_params\": best_params, \n",
    "        \"cv_score\": best_cv_score,\n",
    "        \"val_score\": val_score,\n",
    "        \"test_score\": test_score\n",
    "    })\n",
    "    \n",
    "    # Visualize hyperparameter\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.plot(grid_search.cv_results_['mean_test_score'], label=f'{model_name}')\n",
    "    plt.xlabel(\"Iterations\")\n",
    "    plt.ylabel(\"Mean F1 Score\")\n",
    "    plt.title(f\"Hyperparameter Tuning for {model_name}\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "# Display results for all models\n",
    "results_df = pd.DataFrame(results)\n",
    "print(results_df)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33e2a3e3",
   "metadata": {},
   "source": [
    "**5.3. Discussion on Doublet Information**\\\n",
    "\n",
    "Note:\n",
    "\n",
    "In the preprocessing, we used \n",
    "```python\n",
    "sc.pp.scrublet(adata, batch_key=\"sample_id\")\n",
    "```\n",
    "to compute doublet scores. Whether to include the doublet_score and predicted_doublet in the classification depends on your hypothesis and if you expect doublets to impact cell type classification. You might want to experiment by including or excluding these metrics in your feature set and evaluating any changes in model performance."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
